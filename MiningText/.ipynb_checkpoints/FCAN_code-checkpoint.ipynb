{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCAN Code\n",
    "#### Will Russell \n",
    "November 2017\n",
    "<hr />\n",
    "<div>\n",
    "    <p> This is the text miner code for the article found in FormingClustersAsNeeded.ipynb </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Project4_code import Porter_Stemmer_Python as ps\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def retrieve_text(relativePath=\"./\", fileName=\"sentences.txt\"):\n",
    "    with open(fileName,  newline=\"\\n\") as f:\n",
    "        sentences = f.readlines()\n",
    "    sentences = [x.strip() for x in sentences]\n",
    "    sentences = [x for x in sentences if len(x) > 0]\n",
    "    return sentences\n",
    "\n",
    "def convert_to_lower(sentences):\n",
    "    lowercase_sentences = []\n",
    "    for sentence in sentences:\n",
    "        lowercase_sentences.append(sentence.lower())\n",
    "    return lowercase_sentences\n",
    "\n",
    "def strip_punctuation(word_token_list):\n",
    "    new_list = []\n",
    "    for word in word_token_list:\n",
    "        p2 = re.compile('[\\W_]+')\n",
    "        result = p2.sub(\"\",word)\n",
    "        if len(result) > 0:\n",
    "            new_list.append(result)\n",
    "    return new_list\n",
    "\n",
    "def strip_text(sentences):\n",
    "    new_text = []\n",
    "    for sentence in sentences:\n",
    "        new_text.append(strip_punctuation(sentence))\n",
    "    return new_text\n",
    "\n",
    "def strip_numbers(sentences):\n",
    "    stripped_sentences = []\n",
    "    pattern = re.compile(r'\\d')\n",
    "    for sentence in sentences:\n",
    "        stripped_sentences.append(re.sub(pattern,\"\", sentence))\n",
    "    return stripped_sentences\n",
    "\n",
    "def split_text(text):\n",
    "    tokens = []\n",
    "    for string in text:\n",
    "        tokens.append(string.split())\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(sentences, stop_words):\n",
    "    modified_sentences = []\n",
    "    for sentence in sentences:\n",
    "        modified_sentences.append([x for x in sentence if x not in stop_words])\n",
    "    return modified_sentences\n",
    "\n",
    "def stem_words(sentence_tokens):\n",
    "    pstemmer = ps.PorterStemmer()\n",
    "    stemmed_sentence = []\n",
    "    for token in sentence_tokens:\n",
    "        stemmed_sentence.append(pstemmer.stem(token,0, len(token)-1))\n",
    "    return stemmed_sentence\n",
    "\n",
    "def stem_text(tokenized_sentences):\n",
    "    stemmed_sentences = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        stemmed_sentences.append(stem_words(sentence))\n",
    "    return stemmed_sentences\n",
    "\n",
    "def flatten_text(sentences):\n",
    "    return [token for sentence in sentences for token in sentence]\n",
    "\n",
    "def generate_token_set(tokens):\n",
    "    return set(tokens)\n",
    "\n",
    "def generate_frequency_count_for_sentence(sentence, token_set):\n",
    "    token_hash = {}\n",
    "    for token in token_set:\n",
    "        token_hash[token] = 0\n",
    "        if token in sentence:\n",
    "            token_hash[token] += 1\n",
    "    return token_hash\n",
    "\n",
    "def generate_text_frequency_vector(text, token_set):\n",
    "    tdm_list = []\n",
    "    for sentence in text:\n",
    "        freq_hash = generate_frequency_count_for_sentence(sentence, token_set)\n",
    "        tdm_list.append(freq_hash)\n",
    "    return tdm_list\n",
    "\n",
    "\n",
    "# Get the stop words from stop_words.txt file\n",
    "stop_words = retrieve_text(fileName=\"stop_words.txt\")\n",
    "# Get the sentences from the sentences.txt file\n",
    "sentences = retrieve_text(fileName=\"sentences.txt\")\n",
    "\n",
    "# Convert all sentences to lowercase\n",
    "sentences = convert_to_lower(sentences)\n",
    "# Strip all numbers from the sentences\n",
    "sentences = strip_numbers(sentences)\n",
    "# Tokenize\n",
    "sentences = split_text(sentences)\n",
    "\n",
    "# Strip the punctuation from the text\n",
    "sentence_tokens = strip_text(sentences)\n",
    "\n",
    "# Remove stopwords\n",
    "sentence_tokens = remove_stopwords(sentence_tokens, stop_words)\n",
    "\n",
    "# Perform stemming\n",
    "stemmed_sentences = stem_text(sentence_tokens)\n",
    "\n",
    "# Flatten the text into a single list\n",
    "stemmed_tokens = flatten_text(stemmed_sentences)\n",
    "\n",
    "# Generate the unique token set\n",
    "unique_tokens = generate_token_set(stemmed_tokens)\n",
    "\n",
    "# Generate the frequency vectors from the tokens\n",
    "vector_list = generate_text_frequency_vector(stemmed_sentences, unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'great', 'suit', 'home', 'know', 'famili', 'attain', 'obsolesc', 'necessarili', 'lai', 'interior', 'biolog', 'remodel', 'spiritu', 'minut', 'predict', 'escap', 'area', 'well', 'cosmo', 'term', 'trash', 'king', 'road', 'number', 'averag', 'learn', 'entir', 'near', 'us', 'second', 'rai', 'open', 'paradigm', 'nice', 'inventor', 'driven', 'sentienc', 'pet', 'gallon', 'sedan', 'charg', 'feel', 'full', 'secur', 'space', 'sens', 'freshli', 'rout', 'domain', 'room', 'coin', 'selfawar', 'reason', 'paint', 'percent', 'achiev', 'go', 'finish', 'autonom', 'car', 'deal', 'deep', 'kitchen', 'floor', 'includ', 'central', 'new', 'knowledg', 'speed', 'human', 'cute', 'mile', 'emot', 'understand', 'classi', 'world', 'gener', 'singl', 'fundament', 'lisp', 'ga', 'awai', 'everyth', 'languag', 't', 'throughout', 'combin', 'sort', 'five', 'updat', 'over', 'queen', 'have', 'negoti', 'on', 'two', 'water', 'comput', 'recent', 'describ', 'air', 'up', 'machin', 'system', 'work', 'rent', 'far', 'intellig', 'base', 'live', 'respons', 'three', 'hour', 'hous', 'through', 'size', 'four', 'approv', 'themtwo', 'sewag', 'artifici', 'realiti', 'conveni', 'dryer', 'around', 'bedroom', 'renov', 'lead', 'park', 'per', 'situat', 'experi', 'accidentfre', 'author', 'increas', 'electr', 'kurzweil', 'multipl', 'rang', 'anim', 'automat', 'futur', 'drive', 'john', 'program', 'bed', 'spread', 'round', 'merger', 'via', 'ag', 'larg', 'lap', 'heat', 'tenant', 'mccarthi', 'pound', 'express', 'get', 'selfimprov', 'applianc', 'owner', 'kilomet', 'type', 'year', 'book', 'basement', 'limit', 'bathroom', 'complet', 'newli', 'util', 'wai', 'techniqu', 'test', 'washer', 'major', 'befor', 'back', 'superintellig', 'possibl', 'come', 'veri', 'row', 'townhous', 'locat', 'singular', 'went', 'engag', 'bath', 'eat', 'public', 'musk', 'commonsens', 'travel', 'exce', 'groundwork'}\n"
     ]
    }
   ],
   "source": [
    "#DemonStrate the tokens are Unique\n",
    "print(unique_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accidentfre</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>achiev</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ag</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>air</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0   1   2   3   4   5   6   7   8   9  ...  17  18  19  20  21  \\\n",
       "accidentfre   0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   \n",
       "achiev        0   0   0   0   1   0   0   0   0   0 ...   0   0   0   0   0   \n",
       "ag            0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   \n",
       "air           0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   \n",
       "anim          0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   \n",
       "\n",
       "             22  23  24  25  26  \n",
       "accidentfre   0   0   0   0   0  \n",
       "achiev        0   0   0   0   0  \n",
       "ag            0   1   0   0   0  \n",
       "air           0   0   0   0   1  \n",
       "anim          0   0   0   0   0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets check out our very sparse matrix...\n",
    "df = pd.DataFrame(vector_list)\n",
    "df_transpose = df.transpose()\n",
    "# df.to_csv(\"tdm.csv\", sep=\",\")\n",
    "# df_transpose.to_csv(\"transpose_tdm.csv\", sep=\",\")\n",
    "df.head()\n",
    "df_transpose.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "# generates and returns the euclidean distance between 2 vectors\n",
    "def euclidean_distance(vector1, vector2):\n",
    "    sum_squares = 0\n",
    "    for key in vector1:\n",
    "        sum_squares += (vector1[key] - vector2[key])**2\n",
    "    return math.sqrt(sum_squares)\n",
    "\n",
    "# Gets the weighted average for a list of vectors\n",
    "def get_weighted_average(vector_list):\n",
    "    avg_hash = {}\n",
    "    if(len(vector_list) == 0):\n",
    "        return 0;\n",
    "    for k in vector_list[0]:\n",
    "        avg_hash[k] = 0\n",
    "        for vec in vector_list:\n",
    "            avg_hash[k] += vec[k]\n",
    "        avg_hash[k] = avg_hash[k]/len(vector_list)\n",
    "    return avg_hash\n",
    "\n",
    "# Generates the cluster for a given weighted average vector\n",
    "def generate_FCAN(current_avg_vector, new_vector, n, alpha=1):\n",
    "    new_weighted_avg = {}\n",
    "    for k in new_vector:\n",
    "        new_weighted_avg[k] = generate_w_k(current_avg_vector[k], new_vector[k], n, alpha)\n",
    "    return new_weighted_avg\n",
    "    \n",
    "# \n",
    "def generate_w_k(weighted_vector, new_vector, m, alpha=1):\n",
    "    return ((m*weighted_vector) + (alpha*new_vector))/(m+1)\n",
    "\n",
    "\n",
    "def pull_valid_keys_from_hash(some_hash):\n",
    "    str_list = []\n",
    "    for k in some_hash:\n",
    "        if some_hash[k] != 0:\n",
    "            str_list.append(k)\n",
    "    return str_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check to see that the get_weighted_vector method works correctly\n",
    "# test_df = df.iloc[1:4]\n",
    "# print(test_df.mean(axis=0))\n",
    "\n",
    "# Test against dataframe subset to ensure working correctly\n",
    "#print(get_weighted_average(vector_list[1:4]))\n",
    "\n",
    "# Check Euclidean Distance is calculated correctly\n",
    "# print(euclidean_distance(vector_list[0], vector_list[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_FCAN_clusters(vector_list, min_distance=10):\n",
    "    current_clusters = []\n",
    "    cluster_avgs = []\n",
    "    current_clusters.append([vector_list[0]])\n",
    "    cluster_avgs.append(vector_list[0])\n",
    "    # Go through the vector list\n",
    "    for i,vec in enumerate(vector_list[1::]):\n",
    "        best_cluster_index = None\n",
    "        found_cluster = False\n",
    "        for j, cluster_vec in enumerate(cluster_avgs):\n",
    "            # compare the cluster vec against the new vector\n",
    "            current_distance = euclidean_distance(vec, cluster_vec)\n",
    "            # If the distance is less than the needed Euclidean\n",
    "            if(current_distance < min_distance):\n",
    "                #Set the current min to be the current distance\n",
    "                # TODO: Currently Last fit --> Way to do Best Fit???\n",
    "                current_min = current_distance\n",
    "                best_cluster_index = j\n",
    "                found_cluster = True\n",
    "        if found_cluster:\n",
    "            ## Get the size of the cluster\n",
    "            m = len(current_clusters[best_cluster_index])\n",
    "            current_clusters[best_cluster_index].append(vec)\n",
    "            # Update the cluster centroid\n",
    "            new_cluster_avg = generate_FCAN(cluster_avgs[best_cluster_index], vec, m)\n",
    "            cluster_avgs[best_cluster_index] = new_cluster_avg\n",
    "        else:\n",
    "            # No cluster found --> create a new cluster\n",
    "            current_clusters.append([vec])\n",
    "            cluster_avgs.append(vec)\n",
    "    return current_clusters\n",
    "\n",
    "\n",
    "def display_FCAN_clusters(clusters):\n",
    "    for i,cluster in enumerate(clusters):\n",
    "        print(\"\\n------Printing out contents of cluster {}  : Cluster length : {}--------\".format(i+1,len(cluster)))\n",
    "        for j, row in enumerate(cluster):\n",
    "            row_vals = pull_valid_keys_from_hash(row)\n",
    "            output = \" \".join(row_vals)\n",
    "            print(\"{} : {}\".format(j+1, output))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------Printing out contents of cluster 1  : Cluster length : 7--------\n",
      "1 : road sedan speed mile up hour per type autonom travel\n",
      "2 : second car mile hour per get kilomet\n",
      "3 : road charg achiev mile rang around kilomet test\n",
      "4 : obsolesc biolog escap human sort have machin intellig merger wai musk\n",
      "5 : charg percent go car mile befor up kilomet\n",
      "6 : sedan mile hour around per lap kilomet autonom\n",
      "7 : necessarili learn sentienc machin intellig artifici lead possibl\n",
      "\n",
      "------Printing out contents of cluster 2  : Cluster length : 1--------\n",
      "1 : learn year predict human awai machin far intellig artifici kurzweil futur rai superintellig singular selfimprov exce\n",
      "\n",
      "------Printing out contents of cluster 3  : Cluster length : 15--------\n",
      "1 : larg home remodel eat full room util kitchen rent live size bedroom newli bath\n",
      "2 : larg home interior entir room paint live bedroom freshli\n",
      "3 : home finish hous dryer bedroom four washer basement come row bath\n",
      "4 : pet space back owner three approv park possibl\n",
      "5 : road us driven public car kilomet limit autonom test\n",
      "6 : suit larg king nice queen on size bedroom bed veri\n",
      "7 : mile over accidentfre drive kilomet autonom complet\n",
      "8 : kitchen floor new updat hous renov\n",
      "9 : term inventor coin lisp languag intellig artifici john program mccarthi\n",
      "10 : road car deal number situat autonom\n",
      "11 : averag gallon sedan mile over per drive pound\n",
      "12 : famili home near rout singl conveni bedroom major locat bath\n",
      "13 : spiritu cosmo author throughout describ machin intellig kurzweil spread ag book rai\n",
      "14 : great open kitchen cute classi updat live hous applianc area veri\n",
      "15 : minut road averag second car mile hour around per round lap went\n",
      "\n",
      "------Printing out contents of cluster 4  : Cluster length : 1--------\n",
      "1 : negoti well pet secur includ ga water system base respons electr anim tenant applianc\n",
      "\n",
      "------Printing out contents of cluster 5  : Cluster length : 1--------\n",
      "1 : know paradigm sens everyth knowledg combin five two intellig artifici experi via wai come realiti\n",
      "\n",
      "------Printing out contents of cluster 6  : Cluster length : 1--------\n",
      "1 : attain learn feel domain selfawar reason knowledg emot understand gener t intellig artifici multipl program comput express engag commonsens\n",
      "\n",
      "------Printing out contents of cluster 7  : Cluster length : 1--------\n",
      "1 : lai eat learn air room deep kitchen includ central understand world fundament trash water recent machin through work intellig live themtwo sewag townhous artifici around bedroom increas automat heat comput bathroom techniqu groundwork\n"
     ]
    }
   ],
   "source": [
    "clusters = generate_FCAN_clusters(vector_list,4)\n",
    "display_FCAN_clusters(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
